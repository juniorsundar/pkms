# Literature
	- ![Graph neural networks: A review of methods and applications](../assets/zhou2021_1710400525745_0.pdf)
		- [Annotations]([[hls__zhou2021_1710400525745_0]])
- # Definition
	- Graph neural networks (GNNs) are a class of deep learning methods designed tow work directly with data structured as graphs.
	- Graphs are a powerful way to represent complex relationships between entities
	- Traditional neural networks like CNNs excel at processing grid-like data. GNNs unlock analysis of data where relationship between data points is as important as the data itself.
	- **Key Applications:**
		- NLP
		- Drug discovery
		- Recommender system
		- Traffic prediction
		- Computer vision
- # Importance
	- **Relational Data Power** -  Many real-world problems have rich connections that GNNs can naturally model, unlike traditional machine learning methods.
	- **Permutation Invariance** - GNNs are not sensitive to the order in which the graph data is presented, making them very flexible.
	- **Inductive Learning** - GNNs can generalize to unseen graphs, a key advantage for problems where data structures change over time.
- # Function
	- **Node Embeddings** - Each node (entity) in the graph is assigned an initial vector representation called an embedding. This captures its features.
	- **Message Passing** - Nodes repeatedly exchange messages with their neighbors. These messages update a node's understanding of its place in the graph.
	- **Aggregation** - A node aggregates messages from its neighbors, combining this information with  its own embedding. This forms a new, refined embedding.
	- **Iterative Refinement** - Message passing and aggregation happen multiple times. With each 
	  iteration, a node's representation becomes more informed by its surroundings in the graph.
	- **Tasks** - This process leads to highly expressive representations of nodes, edges, or the whole graph, used for tasks.