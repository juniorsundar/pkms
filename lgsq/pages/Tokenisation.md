# Definition
	- Machine learning models inherently work with numerical representations. Tokenization is the bridge that converts raw text data, which is unstructured and messy, into a format computers can actually process.
	- Tokenization involves splitting text into meaningful units called tokens. These tokens can be:
		- **Words** - "The", "quick", "brown", "fox..."
		- **Subwords** - Word pieces like "jump__ed", allowing models to handle new or out-of-vocabulary words effectively.
		- **Characters** - In some cases, even individual letters.
		- **Sentences** - For tasks where understanding the full context of a sentence is necessary.
	- **Examples**
		- **Sentiment Analysis** - Tokenizing a review allows a model to analyze the positive/negative sentiment associated with specific words or phrases.
		- **Machine Translation** - Tokenized sentences in one language are translated into tokens in the target language, using models trained on aligned text pairs.
		- **Chatbots** - Tokenization allows the chatbot to understand the structure of user queries and generate appropriate responses.
- # Types
	- **Word-level Tokenization:**
		- Simplest form, commonly used.
		- Splits sentences on spaces or punctuation, treating each word as a token.
	- **Subword Tokenization:**
		- Breaks words into smaller units (prefixes, suffixes, roots).
		- Handles rare words and word variations better, leading to more efficient vocabulary representation.
		- Techniques like Byte Pair Encoding (BPE) and WordPiece are popular.
	- **Character-level Tokenization:**
		- Treats individual characters as tokens.
		- Useful for languages without word boundaries or when dealing with very noisy text (like social media data).
	- **Sentence-level Tokenization:**
		- Splits text into complete sentences.
		- Important for tasks like text summarization or question answering, where understanding sentence context is crucial.
- # How it Helps
	- **Feature Creation** - Tokens become numerical features fed into ML models.
	- **Vocabulary Building** -  Tokenization defines the set of unique words or subwords your model will understand.
	- **Sequence Representation** - Preserves word order for tasks where the structure of language matters.
	- **Model Input** - Allows for techniques like word embeddings (e.g., Word2Vec, GloVe) that capture semantic relationships between words.