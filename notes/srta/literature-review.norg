@document.meta
title: Literature Review
description: 
authors: juniorsundar
categories: 
created: 2024-01-28T21:48:57+0400
updated: 2024-02-12T18:10:49+0400
version: 1.1.1
@end

* Runtime Assurance of Aeronautical Products: Preliminary Recommendations

  | /Access {https://ntrs.nasa.gov/api/citations/20220015734/downloads/tm-rta-guidance.pdf}[reference]/

** Introduction

   Runtime assurance (RTA) affords an operational layer of protection against
   safety hazards to systems that may include less trusted or untrusted
   functions. To that end, the RTA scheme must itself be trusted before it can
   be deployed into use:
   ~ It must fit the intended purpose.
   ~ It must not itself introduce safety hazards.

** Definitions

   - Runtime Monitoring (RTM) - observation of an executing system of interest,
     its functions, or its environment.
   - Runtime Verification (RV) - specialisation of RTM, where response of
     monitor is result of an online verification procedure applied to monitor
     inputs.
   - Runtime Assurance (RTM) - combinatin of RTM and one or more functions
     triggered by RTM, such as recovery, failover, warning, or shutdown.
   - System Under Observation (SUO) - system of interest for RTA.
   - Integrated System - SUO that includes RTA.

** Overview

   {/ assets/literature-review1.png}[Figure: Architecture of RTA]

   RTA functions an be decomposed into the following implicitly runtime
   functions:
   ~ *Input assurance* - Ensures that not only do monitoring and backup functions
     receive trusted inputs, but also that they receive the right inputs.
   ~ *Monitoring* - Detects safety-related deviations/violations by observing an
     SUO
      -- emergent interactions at system boundary,
      -- violations or incorrect function inputs or assumptions of
         environmental conditions,
      -- computational deviations from required internal states, state
         changes, and guards in state transitions, or undesired state
         transitions.
   ~ *Switching decision logic* - Risk mitigation intervention triggered by
     monitor to disconnect /complex function/ and engage the /backup
     function/. Can be a simple or more sophisticated protocol.
   ~ *Backup function* - RTA includes one or more that serve to replace or
     failover from the primary complex function. Is generally a copy of primary
     with reduced service and capability but robust safety.

** Design

   Trusted RTA is:
   ~ Simple
   ~ Benign
   ~ Realisable
   ~ Verifiable
   ~ Dependable

* Digital Twin for UAV Anomaly Detection

  | /Access {https://www.duo.uio.no/bitstream/handle/10852/93934/1/DTAnomally.pdf}[reference]/

  The thesis explores the use of unmanned aerial vehicles (UAVs), emphasizing
  their rising popularity due to their low cost and simplicity compared to
  manned aircraft, particularly in dangerous or remote areas. It focuses on
  ensuring UAV safety and reliability in sensitive airspace by detecting sudden
  and anomalous behaviors. The study investigates the use of Inertial
  Measurement Units (IMU) and vibration analysis for monitoring UAV health.

  A {:../technical/terminologies.digital-twin:}[Digital Twin] is employed to simulate physical system
  operations, generating data to identify normal UAV
  operation modes. Real-time operational data is compared against this to
  detect anomalies, using TimeGAN, a GAN variant for time series data, and
  k-Means clustering algorithm.

* Fault Injector for Autonomous Quadrotors)

  | /Access {https://estudogeral.uc.pt/bitstream/10316/95542/1/2021_Master_internship_Artur_Coutinho_Final_Report.pdf}[reference]/
  | GitHub: {https://github.com/djtrak33/DFATool}[djtrak33/DFATool]

** Introduction

   The thesis includes a comprehensive literature review to establish a
   theoretical framework for improving UAV failure prediction, examining
   current UAV capabilities, sensor technology, and the application of digital
   twins in failure detection and predictive maintenance.

** Objectives

   - defining and characterizing the quadcopter and its environment,
   - identifying and characterizing failure scenarios, and
   - creating a fault model encompassing generic and quadcopter-specific
     faults.

   The end goal is to implement these faults using a tool that manages their
   presence in the code.

** Approach

   While the original plan included a risk analysis of the faults, it was later
   discarded due to its perceived misalignment with the practical nature of the
   work. It is also noted that depending on the complexity of implementing
   exploits, it may not be possible to implement all identified faults. The
   project intends to perform experiments related to system assessment in an
   organized and efficient manner.

   The research approach involves:
   - identifying all components of an autonomous quadrotor,
   - building a representative model,
   - selecting appropriate simulation software, and
   - creating a fault model.

   The fault model is developed by analyzing software modules for potential
   exploitation. The document then describes the conceptualization of a fault
   injection tool using a web-based prototyping tool (Figma) and its subsequent
   implementation using Qt Creator.

   The implementation phase involves working on both the tool's interface and
   the incorporation of specific faults, primarily focusing on GPS faults. The
   final step includes performing experiments to validate the implemented
   elements and analyzing the results to draw conclusions about the tool's
   effectiveness.

** Methodology

   The flight-controller stack used is the PX4-Autopilot stack, and the
   simulation environment is Gazebo. This is preferred due to the light-weight
   and open-source nature of both. PX4’s robotics friendly middleware that
   facilitates communication with the flight-controller stack and companion
   systems was attractive. Gazebo’s wide array of sensor implementations were
   attractive for this.

   The PX4’s internal firmware architecture is defined as in the following
   image. (NOTE: there are changes with the newest version of PX4 firmware as
   there is a shift towards the MicroXRCE-DDS firmware over the previous
   methodology).

   {/ assets/literature-review2.png}[Figure: PX4 High-Level Software Architecture Diagram]

   The report defines 6 kinds of faults that can potentially injected into the
   system.
   - *Communication faults*: faults that target the MAVLink communication channel
     between PX4’s MAVLink module and its respective ROS node.
   - *Command faults*: faults that target commands issued by the State Machine
     module (e.g. abort command).
   - *Environmental/Sensors/PS faults*: faults that target the data gathered by
     the sensors and corrupt it.
   - *Mission faults*: faults that target the mission on cache whilst it is being
     read, possibly affecting trajectory.
   - *Sensors/PS data transformation faults*: faults that target the data
     gathered by the sensors whilst it is being processed.
   - *Estimator faults*: faults that target the processed data once it is sent to
     the modules that control the drone’s movement.

   These faults can potentially be implemented as follows:

   {/ assets/literature-review3.png}[Figure: High Level Diagram of Simulated Environment with Targets for Fault Implementation]

   Although the report lists a range of methods to implement faults, it limits
   its work to the GPS module in order to map and observe the behaviour before,
   during and after fault injection. The test cases were for Freeze, Delay and
   Fixed type faults. This is implemented in the {https://github.com/djtrak33/DFATool/blob/fd51eebafb72f4ae162e89de62ebeef0c3e4386d/PX4-Autopilot/src/modules/sensors/vehicle_gps_position/VehicleGPSPosition.cpp}[code] (from Line 885) as follows:

   @code cpp
   if(fault_mode == 1 && !std::isnan(start_injection_time) && !std::isnan(end_injection_time) && (gps.timestamp - takeoff_start_timestamp >= start_injection_time) && (gps.timestamp - takeoff_start_timestamp <= end_injection_time)){
       //Fixed Value
       PX4_INFO("injecting gps fixed value");
       if(!std::isnan(injected_gps_lat))  gps_output.lat = (int32_t) injected_gps_lat; else gps_output.lat = gps.lat;
       if(!std::isnan(injected_gps_lon))  gps_output.lon = (int32_t) injected_gps_lon; else gps_output.lon = gps.lon;
       if(!std::isnan(injected_gps_alt))  gps_output.alt = (int32_t) injected_gps_alt; else gps_output.alt = gps.alt;
   }else if(fault_mode == 2 && !std::isnan(start_injection_time) && !std::isnan(end_injection_time) && (gps.timestamp - takeoff_start_timestamp >= start_injection_time) && (gps.timestamp - takeoff_start_timestamp <= end_injection_time)){
       //Freeze Value
       PX4_INFO("injecting gps freeze value");
       injected_gps_lat = gps.lat;
       injected_gps_lon = gps.lon;
       injected_gps_alt = gps.alt;
       gps_output.lat = injected_gps_lat;
       gps_output.lon = injected_gps_lon;
       gps_output.alt = injected_gps_alt;
       hasFreezeValue = true;
   }else if(fault_mode == 3 && !std::isnan(start_injection_time) && !std::isnan(end_injection_time) && (gps.timestamp - takeoff_start_timestamp >= start_injection_time) && (gps.timestamp - takeoff_start_timestamp <= end_injection_time)){
       //Delay Value
       //Wait for delay_value seconds
       PX4_INFO("injecting gps delay value");
       gps_output.lat = gps.lat;
       gps_output.lon = gps.lon;
       gps_output.alt = gps.alt;
       std::this_thread::sleep_for(std::chrono::seconds((int)delay_value));
   }
   else{
       gps_output.lat = gps.lat;
       gps_output.lon = gps.lon;
       gps_output.alt = gps.alt;
   }
   @end

   The second half of the report defines a prototype UI that can be used to
   inject the faults into the system. 

   PX4 contains various failsafes that allow it to detect anomalies internally
   and abort flights that exhibit said anomalies. In this experiment, the
   failsafe activation delays were set to minimum. Conclusion

   First off, although the failsafe delay was set, any sudden changes in values
   was met with an immediate abort command.

   The report concludes that the initial objectives were met, with the
   behaviour of simlated drone under fault matching expected hypotheses.
   However, there is still room for improvement as not all faults were
   implemented.

** Evaluation

   This report, while extensive in its set-up, falls during the implementation
   stages significantly. The testing is insufficient to conclude that the
   objectives are met. It is suspected that a time-crunch or resource-crunch
   resulted in this level of output. Nonetheless, even with this shortcoming,
   it offers a pathway to feasibly inject faults into the UAV system for
   testing sRTA use cases.

   First off, the injection of fault for GPS is hardcoded into the firmware,
   requiring recompilation everything a new type of fault is expected to be
   injected. To fix this, relying on the internal system parameter server is
   considered. Changing the values in the server for params corresponding to
   fault types can make the fault testing more dynamic in nature.

   Secondly, we can refer to the listed fault classes and expand it by also
   adding a category called Use Case class that refers to the type of use
   case a potential fault can fall under. A rough example list could be:
   Security Attack, Sensor Failure, Controller Failure, Software Failure,
   etc. 

   This can then be put in tabular form like this:

   @code markdown
   | Fault Title                                                                                             | Fault Class                           | Use Case Class                                     |
   |---------------------------------------------------------------------------------------------------------|---------------------------------------|----------------------------------------------------|
   | Target and alter data retrieved from sensors                                                            | Environmental/Sensors/PS faults       | Security Attack/Sensor Failure                     |
   | Alter published flight control output signal values                                                     | Sensors/PS data transformation faults | Software Failure/Controller Failure                |
   | Intercept and alter mission commands and mission signals                                                | Mission faults                        | Security Attack                                    |
   | Target middleware that facilitates communication between controller-processor/drone-drone/drone-GC etc. | Communication faults                  | Security Attack/Software Failure                   |
   | Target and alter calibration of the internal controller                                                 | Estimator faults                      | Controller Failure/Software Failure                |
   | Confound the internal state machine that maintains transition between flight modes and states           | Command faults                        | Software Failure/Controller Failure/Sensor Failure |
   @end

   Finally, observe that most, if not all, of the faults in the thesis are
   targeting the flight control modules within the firmware instead of the
   environmental models, sensors models, or quadcopter models in the simulated
   environment. This has its advantage as the firmware is simulator independent
   and can be run on different simulator types. However, this also means that
   HITL testing with the firmware can get sketchy as additional memory and
   internal clock interruptions are potentially being introduced that will
   affect the hardware's runtime behaviour. 

* Vision Paper: Grand Challenges in Resilience: Autonomous System Resilience through Design and Runtime Measures

  | /Access {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9133332}[reference]/

** Introduction

   $ Resilience by design
   This is the aspect that designs and develops cyber systems so that they are
   resilient to a large set of quantifiable perturbation.

   $ Resilience by reaction
   This is the aspect that works at runtime when perturbations are incident on
   the cyber system and imbues the system with the ability to "bounce back"
   quickly after a failure triggered by a perturbation.

   /Perturbations/ take three forms:

   ~ *Natural failures of hardware or software* - due to bugs, aging,
     misconfigurations, resource contentions in shared environments, downtime
     due to planned upgrades, etc.
   ~ *Maliciously induced failures or security attacks* - from outside the
     system
   ~ *Unexpected inputs* - either through errors or unpredictability

   Outcome of perturbation that is not handled can be a /hard failure/ (crash
   or hang) or /soft failure/ (i.e. missing a deadline for a latency-sensitive
   application).

** Autonomous Systems as Application Drivers

*** Distributed resilience in multi-agent drones for medical deliveries

    *Problem and Current State* - A multiple drone system that is responsible
    for transporting essential supplies to a population affected by a natural
    disaster or medical supplies to a population where the surface
    transportation infrastructure is poor.

    Consensus-based algorithms assume that all agents are cooperative, sharing
    their state values with neighbors and adhering to a common protocol to
    achieve network objectives. However, the autonomous systems using these
    algorithms face challenges due to their operation in open and potentially
    adversarial environments, making them susceptible to a range of
    disruptions.

    While distributed algorithms are generally robust against failures of
    individual nodes or links due to their decentralized nature, their reliance
    on local coordination also makes them vulnerable to cyber attacks targeting
    specific agents. Therefore, ensuring resilience is crucial for the
    successful application of autonomous multi-agent swarms in critical areas.

*** Cooperative autonomous rescue with active adversary

    Consider application of artificial intelligence (AI) in military contexts,
    particularly focusing on operational challenges similar to those in
    civilian rescue operations, such as dealing with dirty and sparse data. It
    highlights a hypothetical military rescue scenario in an urban environment
    to underscore the complexities AI systems face, including rapid
    environmental changes, unstable structures, and the need for situational
    understanding (SU) in the presence of anti-access and area denial (A2AD)
    tactics.

    These challenges necessitate advanced AI capabilities for data fusion from
    diverse sensors, autonomous coordination, and navigating through physically
    compromised structures without reliable communication links. The passage
    also mentions ongoing research efforts aimed at improving sensor
    technologies, sensor fusion, robotic perception, and the integration of
    these technologies into complex systems capable of performing in dynamic
    and uncertain environments.

    Addressing these challenges is crucial not only for military operations but
    also for enhancing the efficacy of civilian rescue missions in disaster
    scenarios, thereby reducing risks to human rescuers and shortening rescue
    timelines.

** Resilience by Design

*** Attacks against building blocks of autonomous systems

**** Current State

     Recent research has shown that adversarial examples, specifically designed
     to fool machine learning models, pose a genuine threat to real-world
     systems. Even when an attacker has limited knowledge of a model's inner
     workings (a black-box scenario), they can successfully craft adversarial
     examples. Techniques include training a substitute model using synthetic
     data or approximating the target model's decision-making process.

     This highlights the transferability of adversarial examples, where attacks
     designed for one model often work against others.  A significant focus lies
     in developing physically realizable adversarial examples that target
     autonomous vehicles. These must remain effective despite variations in
     depth, angle, and lighting conditions that self-driving cars encounter.

     Researchers are adapting attack methods to include image transformations,
     making adversarial examples more resilient to the challenges of real-world
     environments.

**** Desired End State and Ways to Get There

     The goal for autonomous systems driven by machine learning is to be robust
     against both intentionally crafted adversarial examples and unexpected,
     "out-of-distribution" inputs that fall outside the system's training data.
     Current open-world machine learning techniques, which attempt to identify
     out-of-distribution samples, have proven vulnerable to adversarial
     attacks.

     Instead, a promising approach focuses on detecting whether hidden layers
     within the model are processing unusual data. While defense strategies
     against adversarial examples are developing, many are tailored to specific
     machine learning models and assume the attacker's strategy is known.

     An ideal approach would make machine learning algorithms fundamentally
     resilient to both intentional attacks and unexpected inputs. Key factors
     in developing such resiliency include:

     - the ease of detecting attacks,
     - effectively using secret information (like cryptographic keys) to
       protect the model, and
     - increasing the computational cost required for effective attacks, even
       if the attacker has full system knowledge.

*** Resilient ML Algorithms

**** Current State

     Machine learning (ML) is increasingly used in critical systems like
     self-driving cars, but ML models can behave unexpectedly due to issues
     like biased training data.  Attackers can exploit these flaws, or
     accidents can occur even without malicious intent, as seen in the Tesla
     autopilot example. Traditional ML testing often can't cover enough
     scenarios to be truly effective in real-world systems.

     Researchers are developing new testing and verification tools to
     systematically find potential problems in ML systems. The key idea is
     "whitebox" testing, which uses the internal workings of the ML model to
     guide the process.  The authors of this text have been creating such tools
     with a focus on analyzing both static and dynamic aspects of ML systems.

     These analysis methods work well for traditional software, but ML systems
     pose unique challenges. Unlike hand-coded logic, ML system logic is
     learned from data. ML is highly non-linear, and it can even be hard to
     define expected behaviors in complex autonomous systems.  Therefore,
     tools focusing on analyzing desirable/undesirable ranges of behavior can
     be beneficial.

     The authors have spent the last few years developing testing and
     verification tools specifically for the complexities of deep learning (DL)
     systems. Because specifying exact behavior for these systems is difficult,
     their tools focus on verifying properties like robustness to slight input
     changes (ex: a change in lighting shouldn't alter image classification).
     They balance scalability, completeness, and soundness in their tool
     design.  These tools have found numerous issues in a variety of real-world
     ML applications and demonstrate impressive performance gains over existing
     solutions. The researchers note that their work is already starting to be
     adopted by both industry and other research teams.

**** Desired End State and Ways to Get There

     While initial results in the field of deep neural network (DNN)
     verification are encouraging, there are significant challenges yet to be
     solved. Current tools excel at verifying properties for specific test
     samples, but there's an assumption that those guarantees will hold for
     unseen data. To make tools more robust, techniques like interval analysis
     could be adapted to reason about entire distributions of possible inputs,
     making fewer assumptions about specific samples.

     Expanding the scope of DNN verification is crucial. This includes
     developing tools that can handle a wider variety of safety properties,
     support different neural network architectures (like Recurrent Neural
     Networks, or RNNs), and work with a broader range of activation functions
     (such as Sigmoid and tanh).

** TO BE CONTINUED

     ===
___

{:./index:}[< return] -  {:$/index:}[index]
