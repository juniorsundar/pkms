@document.meta
title: 19 February 2024
description: 
authors: juniorsundar
categories: 
created: 2024-01-29T11:08:22+0400
updated: 2024-02-20T20:17:36+0400
version: 1.1.1
@end

* To-Do

  - (-) Flash custom firmware into Saluki V2
     -- (x) Flash faulty-firmware and set up HITL
     -- (-) Test Fault Injection
     -- ( ) Test control of flight through MAVSDK
     -- ( ) Touch base with Yashraj to determine what is needed from the HiTL
  - ( ) Discuss how to implement the fault injection functionality in dronsole
     -- ( ) Check if Esko and Ilkka need to be invited to Meeting
  - (-) Meeting with Siva
     -- ( ) Hold meeting
  - (-) Collect data for Samridha
     -- (x) Fix bug with the fault-state being active even when no faults are present
     -- ( ) Produce some data that causes crashing of drone
  - (-) 22nd February, PPT slides CalTech syncup
     -- (-) Trim PPT
     -- ( ) Prepare script
  - ( ) 23rd February, Share Objectives (meet | exceed | percentage)

* Journal

  - {:$/technical/px4/simulation:* Hardware-in-the-Loop (HITL)}[HITL set up] with Saluki V2
  - {* Bug Fix in SITL Pipeline}[Fixed bug] in SITL pipeline
  - Updated {:$/tgt/index:}[The Good Teacher]
     -- Completed {:$/tgt/volume-6/chapter-422:}[Chapter 422 - Fear, Uncertainty, Doubt]
     -- Uploaded {:$/tgt/volume-6/chapter-410:}[Chapter 410 - Regrouping]


  ===
___

* Bug Fix in SITL Pipeline

  The fault was being recorded as activated since the trigger did not check to
  see if there were any active faults to begin with. The issue was identified
  in {:$/technical/px4/simulation.use-case-sensor-fault-injection-simulation:**** fault_manager.py}[fault_manager.py].

  This was fixed by checking if there were any faults to activate before
  appending the active flag to `.csv`. Observe relative lines 10-14 below:

  @code python
  def _activate_faults(self):
      """
        Activates faults based on the current configuration.
        """
      self.faults_active = True
      self.out_string += f"{int(self.get_clock().now().nanoseconds / 1000)},"

      # Fault must be registered as active iff it has at least one active fault
      if len(list(self.faulty_sensors.keys())) == 0:
          self.out_string += "0,"
      else:
          self.out_string += "1,"

      for key in list(self.faulty_sensors.keys()):
          sensor = self.faulty_sensors[key]
          for fault_type in list(sensor.keys()):
              if fault_type == 'activator':
                  self.param_int_pub.publish(String(data = f"{sensor[fault_type]}/{1}"))
              else:
                  random_value = random.random() * (sensor[fault_type]['vals'][1] - sensor[fault_type]['vals'][0]) + sensor[fault_type]['vals'][0]
                  self.param_float_pub.publish(String(data = f"{sensor[fault_type]['label']}/{random_value}"))
                  self.out_string += f"{random_value},"
      self.out_string = self.out_string[:-1] + "\n"
  @end

  ===
___

* Meeting: Datahub and MLOps

** Attendees

   | SRTA-AD
   | Solita
   | | Moamen Ibrahim
   | | Rodolfo Pedraza
   | | Jarmo Hakala

** Goals

   - Datahub
   - MLOps

** Agenda

   - (x) What is Datahub
   - (x) Datahub building blocks
   - (x) Datahub overall architecture
   - (x) Query data
   - (x) Datahub metadata and use cases
   - (x) NATSAPI usage
   - (x) SRTA and MLOps

** Minutes

   /Moamen Ibrahim/
   - Datahub aggregates metadata from missions and devices. Provides
     centralised system for data.
   - Can see data flows.
   - Collects and shares data in variety of formats.
   - Track data collected by devices and build relationship between those
     entities.
   - Datahub:
      ~~ Gateway aggregator: Listens to mission and telemetry data and publish
         them for storing mission info
      ~~ NATS-API: Like FAST-API but for NATS based applications
      ~~ Datahub UI: User-friendly access to data
      ~~ Database: Stores in PostGRESQL
   - Datahub offline only has access to metadata
   - Datahub cannot truly be real-time
   - Data Ingestion: ROSBag decryption pipeline
   - Data Recorder: Define which topics are to be recorded and what
     size/parameter. It will record and dump ROSBags according to that
     provision.
   - /Every drone in swarm is sending data to cloud. Not using the current
   swarm architecture of micro-, meso-, macro-level./
   - *Data format for being processed and used by ML Models??*
   - {https://ssrc.atlassian.net/wiki/spaces/ML/pages/963903933/DataHub+Use+Cases}[DataHub Use Cases]
   - Method of querying the data more efficiently. Crash by crash, etc...

   /Jarmo Hakala & Rodolfo Pedraza/
   - {https://github.com/tiiuae/platform-ml-rta-sample-model}[Model Development]
   - mlflow model registry?
   - Data preprocessing from NATS and ROS interface

** Action Items
   - (x) Check how the data is being received and recorded so that it can be used by ML models. The format, processing, etc.
   - ( ) Check if it is possible to automate ROS node generation.

   ===
___

{:$/journal/2024/02/18:}[< previous] - {:$/journal/index:}[index] - {:$/journal/2024/02/20:}[next >]
